{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e005adb4-effc-448e-b376-3467fe05187d",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98be0cbd-e337-4ee7-977c-01a9e60a886d",
   "metadata": {},
   "source": [
    "In this notebook I will use a convolutional neural network to determine what string on the guitar a note was played on. To achieve this I will be using a dataset consisting of spectrogram images produced by wav files of single notes being played on guitar. Finding an accurate way to determine what string a note was played on will solve the issue of the same \"exact\" note existing multiple places on the guitar, with only slight tonal differences distinguishing them. This approach aims incorporate the tonal differences of the notes by looking at the spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd1800f-14c2-4d91-a2b9-23f8955ac6da",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cb6cd2-1ca7-4347-ac6e-daae19b891de",
   "metadata": {},
   "source": [
    "Deriving the spectograms involved recording 100s of audios and then using librosa to convert the wav files to spectograms. The recording process was done using my own electric guitar, an audio interface, and the ableton lite recording software. The tone is completely clean, meaning the input of the guiitar is not being modified in any way. Each fret was recorded for each string, when the end was reached (meaning all frets were recorded) I would down tune the guitar to make sure the notes are distinct and increase the overlap in notes with same pitch but different strings. I then created a script to convert the audios to spectograms, trimming the beginning and end to make sure there is no external sound (finger going on/off fret)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55381987-bcef-4b5a-a969-e3b66d7b3502",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b82bb3a-cff9-4751-be57-f7fda92da94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "image_directory = \"C:/Users/Mario/PycharmProjects/SoundTesting/Spectrograms\"\n",
    "\n",
    "y = []\n",
    "X = []\n",
    "\n",
    "for file in os.listdir(image_directory):\n",
    "    string = file[0]\n",
    "    pixels = cv2.imread(image_directory + '/' + file)\n",
    "    X.append(pixels)\n",
    "    if string == 'E':\n",
    "        y.append(0)\n",
    "    elif string == 'A':\n",
    "        y.append(1)\n",
    "    elif string == 'D':\n",
    "        y.append(2)\n",
    "    elif string == 'G':\n",
    "        y.append(3)\n",
    "    elif string == 'B':\n",
    "        y.append(4)\n",
    "    elif string == 'H':\n",
    "        y.append(5)\n",
    "y = np.array(y)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4cf50d0-1fbc-4eec-b45d-e90b977b3c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "823\n",
      "823\n"
     ]
    }
   ],
   "source": [
    "print(len(X))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa27423-f3ae-46b3-8d68-52d6468b538f",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "430bbf61-895f-4ff3-8e56-3a80dc48f95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define the CNN architecture\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(200, 500, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(6, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97467cb7-2083-408f-8f01-5028e2b20d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7851a3ba-be49-43d4-8eac-6b02f80a1909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b77140f3-a0f2-40d0-8b5c-3763aff577e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_9 (Conv2D)           (None, 198, 498, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPoolin  (None, 99, 249, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 97, 247, 64)       18496     \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 48, 123, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 46, 121, 64)       36928     \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 356224)            0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                22798400  \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22855110 (87.19 MB)\n",
      "Trainable params: 22855110 (87.19 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 82s 7s/step - loss: 0.0603 - accuracy: 0.9818 - val_loss: 0.6313 - val_accuracy: 0.8788\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 49s 4s/step - loss: 0.0320 - accuracy: 0.9939 - val_loss: 0.6695 - val_accuracy: 0.8485\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 81s 8s/step - loss: 0.0276 - accuracy: 0.9939 - val_loss: 0.6492 - val_accuracy: 0.8364\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 50s 5s/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 0.7322 - val_accuracy: 0.8606\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 50s 5s/step - loss: 0.0161 - accuracy: 0.9954 - val_loss: 0.5882 - val_accuracy: 0.8545\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 51s 5s/step - loss: 0.0240 - accuracy: 0.9939 - val_loss: 0.6760 - val_accuracy: 0.8606\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 52s 5s/step - loss: 0.0142 - accuracy: 0.9970 - val_loss: 0.6130 - val_accuracy: 0.8606\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 51s 5s/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.5962 - val_accuracy: 0.8667\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 50s 4s/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.6342 - val_accuracy: 0.8727\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 50s 5s/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.6867 - val_accuracy: 0.8727\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0160633-6885-4f68-9a66-635e1ac9a093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 370ms/step - loss: 0.6867 - accuracy: 0.8727\n",
      "Test Accuracy: 0.8727272748947144\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce9511a6-f67b-49c3-88d8-ea19c0816a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 2s 337ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.91        26\n",
      "           1       0.90      0.87      0.89        31\n",
      "           2       0.88      0.94      0.91        32\n",
      "           3       0.96      0.73      0.83        30\n",
      "           4       0.73      0.90      0.81        21\n",
      "           5       0.88      0.88      0.88        25\n",
      "\n",
      "    accuracy                           0.87       165\n",
      "   macro avg       0.87      0.87      0.87       165\n",
      "weighted avg       0.88      0.87      0.87       165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "report = classification_report(y_test, y_pred_classes)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59df933d-4f80-447d-a45d-a53707b4b8d8",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0b0b26-2339-47ea-bb82-c5b43fa1636c",
   "metadata": {},
   "source": [
    "Looking at the results we can see that the model had an accuracy of 89% meaning it is classifying correctly a majority of the time. It's also worth noting that there are 6 different classes making this level of accuracy very impressive. Looking at the precision we can safely say that it is predicting well for each class and is giving few false positives with the precision ranging from 70% to 96% across the different strings. Finally, the high recall (70% to 96%) indicates that it is also recognizing almost all of the positive instances in addition to giving few false positives. Overall, using spectograms to identify slight tonal differences in similar sounds seems to be a viable approach and can be used for other cases that involves using tone/frequencies to identify something."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707cbb85-c087-471d-af1d-9bd53d8bf072",
   "metadata": {},
   "source": [
    "# Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8501932f-4e5e-4750-af5f-ec5cd1fe7131",
   "metadata": {},
   "source": [
    "In terms of improvements, there are various things that can be worked on. The biggest limitation would be hardware resources, currently the image size is fairly small for it to run on my system. With a better system I could use larger images which would provide the model with better details about the spectogram. Another thing to consider is that every guitar is unique and has its own distinct sound, since the audios were recorded only on my guitar it's likely it will only perform well on my guitar. Having a more diverse set of recordings across various different guitars will improve the generalization of the model. In addition, including audios with different tones/added effects can also increase the generalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
